apply plugin: 'download-task'

buildscript {
    repositories {
        mavenLocal()
        mavenCentral()
        jcenter()
        maven { url 'https://oss.sonatype.org/content/repositories/snapshots' }
        maven { url 'https://github.com/ben-manes/gradle-versions-plugin/raw/mvnrepo' }
    }
    dependencies {
        classpath 'org.zeroturnaround:zt-exec:1.6'
        classpath 'org.apache.commons:commons-lang3:3.3.2'
        classpath 'de.undercouch:gradle-download-task:1.0'
    }
}

task wrapper(type: Wrapper) << {
    gradleVersion = '2.2'
}

ext.userHome = System.properties['user.home']
ext.profile = file("${userHome}/.profile")
ext.workshop_files = file("${project.buildDir}/workshop-files")

ext.hadoop_version = '2.6.0'
ext.hadoop_home = file("${workshop_files}/hadoop-${hadoop_version}")
ext.hadoop = "${hadoop_home}/bin/hadoop".toString()
ext.hdfs = "${hadoop_home}/bin/hdfs".toString()

ext.hive_version = '1.0.0'
ext.hive_home = file("${workshop_files}/apache-hive-${hive_version}-bin")
ext.hive = "${hive_home}/bin/hive".toString()

ext.pig_version = '0.13.0'
ext.pig_home = file("${workshop_files}/pig-${pig_version}")
ext.pig = "${pig_home}/bin/pig".toString()

ext.spark_version = '1.2.1'
ext.spark_home = file("${workshop_files}/spark-${spark_version}")
ext.spark = "${spark_home}/bin/spark-submit".toString()

def log(message) {
    logger.warn "[----------- ${message} -----------]"
}

def fileContains(file, content) {
    file.exists() && file.getText().contains(content)
}

def append(file, content) {
    file << content + "\n"
}

def downloadFile(path) {
    def tmpDir = new File(System.properties['java.io.tmpdir'])
    def file = new File(tmpDir, new File(new URL(path).getPath()).getName())

    def count = 0;
    while (!file.exists()) {
        try {
            println "Downloading from ${path} to ${file}."
            download {
                src path
                dest file
                onlyIfNewer true
            }
        } catch (Exception e) {
            println "Extraction failed: " + e.getMessage()
            e.printStackTrace()
            file.delete()
        }
        if (count++ > 0) {
            throw new GradleException("Failed to download after 3 attempts: ${path}");
        }
    }
    return file
}

def extract(destination, file, target) {
    if (!target.exists()) {
        destination.mkdirs()
        println "Extracing ${file} to ${destination}"
        copy {
            from(tarTree(file))
            into destination
        }
    }
}

task clean(type: Delete) {
    delete project.buildDir
}

def execute(command, args = [], outStream = null, errStream = null, background = false) {
    def env = []

    env << System.getProperties()
/*
    showType('command', command)
    args.forEach {
        showType('arg', it)
    }
    showType('outStream', outStream)
    showType('errStream', errStream)
    showType('background', background)
*/
    
    def executor = new org.zeroturnaround.exec.ProcessExecutor().command([command.toString()] + args)
                                                                .readOutput(true)
                                                                .environment(env);
    if (outStream) {
        outStream.getParentFile().mkdirs()
        executor.redirectOutput(new FileOutputStream(outStream, true));
    } else {
        executor.redirectOutput(System.out);
    }
    if (errStream) {
        errStream.getParentFile().mkdirs()
        executor.redirectError(new FileOutputStream(errStream, true));
    }

    if (!background) {
        try {
            def result = executor.execute();
            if (outStream == null) {
                return result.outputString().split('\n')
            }
        } catch (Exception e) {
            e.printStackTrace()
        }
    } else {
        executor.start()
    }
}

def showType(name, value) {
    print "name: ${name} --> "
    if (value) {
        print value.class.getName()
        print " - "
    }
    println value
}

task install_dataset() << {
    def extracted = file("${workshop_files}/mlsmall")
    extract workshop_files, file('dataset/mlsmall.tar.gz'), extracted
    execute "mongorestore", ['--drop', '-d', 'movielens', extracted.getPath()]
}

task install_hadoop(dependsOn: "init") << {
    def hadoop = downloadFile "https://archive.apache.org/dist/hadoop/common/hadoop-${hadoop_version}/hadoop-${hadoop_version}.tar.gz"


    extract workshop_files, hadoop, hadoop_home
    append file("${hadoop_home}/etc/hadoop/hadoop-env.sh"), "export JAVA_HOME=${System.properties['java.home']}"
    writeTo(file("${hadoop_home}/etc/hadoop/core-site.xml"), "<configuration>\n" +
                                                             "\t<property>\n" +
                                                             "\t\t<name>fs.defaultFS</name>\n" +
                                                             "\t\t<value>hdfs://localhost:9000</value>\n" +
                                                             "\t</property>\n" +
                                                             "</configuration>")
    writeTo(file("${hadoop_home}/etc/hadoop/hdfs-site.xml"), "<configuration>\n" +
                                                             "\t<property>\n" +
                                                             "\t\t<name>dfs.replication</name>\n" +
                                                             "\t\t<value>1</value>\n" +
                                                             "  \t</property>\n" +
                                                             "</configuration>")
    execute hdfs, ["namenode", "-format", "-force"], null,
            file("${workshop_files}/logs/namenode-format.out")

    execute "${hadoop_home}/sbin/start-dfs.sh"
    execute "${hadoop_home}/sbin/start-yarn.sh"
}

task install_hive(dependsOn: ["init", "install_hadoop"]) << {
    def hive = downloadFile "https://archive.apache.org/dist/hive/hive-${hive_version}/apache-hive-${hive_version}-bin.tar.gz"
    extract workshop_files, hive, hive_home

    execute hadoop, ["fs", "-mkdir", "/tmp"]
    execute hadoop, ["fs", "-mkdir", "-p", "/user/hive/warehouse"]
    execute hadoop, ["fs", "-chmod", "g+w", "/tmp"]
    execute hadoop, ["fs", "-chmod", "g+w", "/user/hive/warehouse"]
    execute hadoop, ["fs", "-mkdir", "-p", "hdfs://localhost:9000/usr/local/hive/lib/"]
    execute hadoop,
            ["fs", "-put", "/usr/local/hive/lib/hive-builtins-0.10.0.jar", "hdfs://localhost:9000/usr/local/hive/lib/hive-builtins-0.10.0.jar"]

    execute "${hive_home}/bin/hive", [], file("${workshop_files}/logs/hive.out"), file("${workshop_files}/logs/hive.out"), true
}

task install_pig(dependsOn: ["init", "install_hadoop"]) << {
    def pig = downloadFile "http://apache.arvixe.com/pig/pig-${pig_version}/pig-${pig_version}.tar.gz"
    extract workshop_files, pig, pig_home
}

task install_spark(dependsOn: ["init", "install_hadoop", "install_dataset"]) << {
    def spark = downloadFile "http://d3kbcqa49mib13.cloudfront.net/spark-${spark_version}-bin-hadoop2.4.tgz"
    extract workshop_files, spark, spark_home

    execute hadoop, ["fs", "-mkdir", "hdfs://localhost:9000/movielens/"], file("${workshop_files}/logs/spark.out")
    execute hadoop, ["fs", "-put", "${workshop_files}/mlsmall/movies.bson".toString(), "hdfs://localhost:9000/movielens/"],
            file("${workshop_files}/logs/spark.out")
}

def writeTo(file, content) {
    def fw2 = new FileWriter(file)
    try {
        fw2.write(content)
        fw2.write("\n")
    } finally {
        fw2.close()
    }
}

task init() << {
    file(workshop_files).mkdirs()
    System.properties.HADOOP_PREFIX = hadoop_home.getAbsolutePath()
    System.properties.HIVE_HOME = hive_home.getAbsolutePath()
    System.properties.PIG_HOME = pig_home.getAbsolutePath()
    System.properties.SPARK_HOME = spark_home.getAbsolutePath()
}
